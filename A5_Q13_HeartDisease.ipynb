{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39e3291e-860b-40ec-92a1-48cae63a3b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPerform the following operations using Python on Heart Diseases data sets  \\na. Data cleaning(Remove NA, ?, Negative values etc.)  \\nb. Error correcting(Outlier detection and removal)  \\nc. Data transformation  \\nd. Build Data model using regression and kNN methods and compare accuracy of \\nheart disease prediction.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Perform the following operations using Python on Heart Diseases data sets  \n",
    "a. Data cleaning(Remove NA, ?, Negative values etc.)  \n",
    "b. Error correcting(Outlier detection and removal)  \n",
    "c. Data transformation  \n",
    "d. Build Data model using regression and kNN methods and compare accuracy of \n",
    "heart disease prediction.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9f98ca36-5737-408d-84de-af0d2ede4110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6c48c1c5-3287-4438-ac30-d8ae9f034afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataset paths\n",
    "files = [\n",
    "    r\"DSBDALExam DataSets\\HeartDisease\\Cleavland.csv\",\n",
    "    r\"DSBDALExam DataSets\\HeartDisease\\hung.csv\",\n",
    "    r\"DSBDALExam DataSets\\HeartDisease\\Switzerland.csv\"\n",
    "]\n",
    "\n",
    "# Define column names based on known attributes used in past experiments\n",
    "columns = [\n",
    "    \"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\", \n",
    "    \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"target\"\n",
    "]\n",
    "\n",
    "# Combine datasets\n",
    "df_list = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(file, header=None, names=columns, na_values='?')\n",
    "    df_list.append(df)\n",
    "\n",
    "data = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "11f0a3a3-2bf4-4468-b960-554048379f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  sex   cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "0  63.0  1.0  1.0     145.0  233.0  1.0      2.0    150.0    0.0      2.3   \n",
       "1  67.0  1.0  4.0     160.0  286.0  0.0      2.0    108.0    1.0      1.5   \n",
       "2  67.0  1.0  4.0     120.0  229.0  0.0      2.0    129.0    1.0      2.6   \n",
       "3  37.0  1.0  3.0     130.0  250.0  0.0      0.0    187.0    0.0      3.5   \n",
       "4  41.0  0.0  2.0     130.0  204.0  0.0      2.0    172.0    0.0      1.4   \n",
       "\n",
       "   slope   ca  thal  target  \n",
       "0    3.0  0.0   6.0       0  \n",
       "1    2.0  3.0   3.0       2  \n",
       "2    2.0  2.0   7.0       1  \n",
       "3    3.0  0.0   3.0       0  \n",
       "4    1.0  0.0   3.0       0  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3724a7e5-cd68-44ca-8afb-4e7813826af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all columns to numeric\n",
    "data = data.apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dfe4d994-ecca-460e-993a-5b755d937126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§¹ Missing Values:\n",
      "age           0\n",
      "sex           0\n",
      "cp            0\n",
      "trestbps      3\n",
      "chol         23\n",
      "fbs          83\n",
      "restecg       2\n",
      "thalach       2\n",
      "exang         2\n",
      "oldpeak       6\n",
      "slope       207\n",
      "ca          413\n",
      "thal        320\n",
      "target        0\n",
      "dtype: int64\n",
      "\n",
      "ðŸ“‰ Negative Values in Columns:\n",
      "oldpeak    11\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nðŸ§¹ Missing Values:\")\n",
    "print(data.isna().sum())\n",
    "\n",
    "# Check for negative values\n",
    "print(\"\\nðŸ“‰ Negative Values in Columns:\")\n",
    "negative_counts = (data < 0).sum()\n",
    "print(negative_counts[negative_counts > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0f09a88b-5d4b-49d6-95a1-83af699fa880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with too many missing values (optional threshold)\n",
    "data = data.dropna(thresh=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c544d184-0011-4217-9b4a-44934f1b8ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill remaining missing values with median\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "data_imputed = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0975b3c9-4604-45c9-8eee-02b7fbf45e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers using IQR\n",
    "def remove_outliers(df):\n",
    "    Q1 = df.quantile(0.25)\n",
    "    Q3 = df.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    return df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "data_cleaned = remove_outliers(data_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fd8a50d3-0569-4210-a661-bc6d1aa0cff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš¨ Rows after outlier removal: 398 (from 716)\n"
     ]
    }
   ],
   "source": [
    "# Print number of outliers removed\n",
    "print(f\"\\nðŸš¨ Rows after outlier removal: {data_cleaned.shape[0]} (from {data_imputed.shape[0]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c1944d17-b7f8-4932-b15e-a489a8cb5591",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\AppData\\Local\\Temp\\ipykernel_24872\\2054581391.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_cleaned['target'] = data_cleaned['target'].apply(lambda x: 1 if x > 0 else 0)\n"
     ]
    }
   ],
   "source": [
    "# Convert target to binary (0: no disease, 1: disease)\n",
    "data_cleaned['target'] = data_cleaned['target'].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4bc980cf-9bd3-4733-9dfb-3d6c67df356d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and target\n",
    "X = data_cleaned.drop('target', axis=1)\n",
    "y = data_cleaned['target']\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "14044af8-7395-4dd7-90e7-e3b71b281a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "003aa972-9345-4a4b-a843-4be1c25f6a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr_pred = lr.predict(X_test)\n",
    "lr_acc = accuracy_score(y_test, lr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ed5bd610-6b7a-4578-9c80-50eecac6c2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "knn_pred = knn.predict(X_test)\n",
    "knn_acc = accuracy_score(y_test, knn_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9183999e-1409-49be-bb85-81b46648aed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy:  0.75\n",
      "KNN Accuracy:  0.8\n"
     ]
    }
   ],
   "source": [
    "# Print Results\n",
    "print(\"Logistic Regression Accuracy: \", lr_acc)\n",
    "print(\"KNN Accuracy: \", knn_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
